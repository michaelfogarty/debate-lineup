---
title: "The Democratic Debate Lineup Isn't *that* Skewed"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
author: "Michael Fogarty"
date: "6/18/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

library(tidyverse)  ## general data manipulation
library(rvest)      ## web scraping
library(randomizr)  ## block random assignment
library(knitr)
library(kableExtra)
library(assertthat)

```

```{r read_data}

debates <- read_rds('debates.RDS')

simulated_data <- read_rds('permutation_simulation_results.RDS')

```

```{r permutation_stat}
permutation_stat <- 
  debates %>% group_by(night) %>% summarize(total = sum(avg)) %>% 
  mutate(diff = abs(total - lag(total))) %>% 
  filter(!is.na(diff)) %>% select(diff) %>% magrittr::extract2(1)
```



## The Debate Lineups

Last Friday, the DNC released the lineups for the first two Democratic debates on June 26 and 27th. To avoid the backlash that followed the Republican primary debates in 2015 when the RNC divided its candidates into a top-tier and JV debate, the DNC decided to randomize the lineups for the two nights.

```{r lineup night 1}
debates %>% 
  filter(night == "June 26") %>% 
  select(candidate, avg) %>% 
  kable(col.names = c('Candidate', 'Polling Average'),
        caption = 'Night 1') %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), position = "center",
                full_width = F)
```

Elizabeth Warren stands out as the lone "top-tier" candidate on the first night (although a case could be made for Beto O'Rourke's status as a major contender as well), while the second debate is comparatively stacked with Joe Biden, Bernie Sanders, Kamala Harris and Pete Buttigieg.

```{r lineup night 2}
debates %>% 
  filter(night == "June 27") %>% 
  select(candidate, avg) %>% 
  kable(col.names = c('Candidate', 'Polling Average'),
        caption = 'Night 2') %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), position = "center",
                full_width = F)
```

News analysis and commentators have called attention to the apparent imbalance between the two nights. Nathaniel Rakich and Geoffrey Skelley at FiveThirtyEight called the debate [lopsided](https://fivethirtyeight.com/features/the-dnc-tried-to-avoid-a-lopsided-debate-it-got-one-anyway/). I argue that, while the lineups certainly aren't balanced, that this result is well within the range of what we'd expect, given that the candidates are assigned randomly to the two debates.

## Just How Imbalanced is this Lineup?

On the surface, it seems like there is a pretty big difference in the polling averages of the candidates on the first and second nights. The mean polling average for the candidates on the first night is `r debates %>% filter(night == 'June 26') %>%  summarize(mean(avg))`%, while the mean for the second night is `r debates %>% filter(night == 'June 27') %>%  summarize(mean(avg))`%. The total polling average (the sum of all of the candidates' averages for each night) also appear quite different: `r debates %>% filter(night == 'June 26') %>%  summarize(sum(avg))`% for the first night versus `r debates %>% filter(night == 'June 27') %>%  summarize(sum(avg))`% for the second.

However, the important question isn't whether or not this difference appears large or small out of context, but whether it is truly out of the range that we would expect given the random assignment of candidates to the two debates. To assess this, I used [permutation inference](https://thomasleeper.com/Rcourse/Tutorials/permutationtests.html), a statistical technique that builds the underlying distribution of possible debate lineups through simulation, then compared the actual outcome to the distribution of possible outcomes. Permutation tests are attractive because they don't make any assumptions about the underlying structure of the data and don't restrict the test statistic.

To measure how skewed the debate lineup is, I decided to focus on the difference in total polling average across the two nights (a similar analysis could be conducted for differences in means or medians as well). I simulated 10,000 random debate lineups and calculated the difference in total polling average between the two nights for each simulated lineup. However, the DNC did not use a simple random assignment mechanism, but rather a blocked assignment, where [the candidates were first split into two categories: those polling at or above 2% and those polling below 2%](https://www.vox.com/2019/6/14/18678241/democratic-presidential-debate-2019-candidates-nights). The candidates were then randomly assigned to one of the two debate nights *within* their category.

```{r histogram, fig.align = 'center'}
hist <- 
ggplot(data = simulated_data, aes(x = dist)) + 
  geom_histogram(bins=100,  fill = 'gray30') +
  geom_vline(xintercept = permutation_stat, color = 'blue', size=1.5) +
  theme_bw() +
  xlab('Difference in Total Polling Average') + ylab('Count') +
  ggtitle('Simple Random Assignment')

hist_block <- ggplot(data = simulated_data, aes(x = block_dist)) + 
  geom_histogram(bins=100,  fill = 'gray30') +
  geom_vline(xintercept = permutation_stat, color = 'blue', size=1.5) +
  theme_bw() +
  xlab('Difference in Total Polling Average') + ylab('Count') +
  ggtitle('Block Random Assignment')

ggpubr::ggarrange(hist, hist_block, ncol=2)
```

The figure on the left shows the distribution of possible debate lineups for totally random assignment mechanism, while the figure on the left shows the result for the stratified assignment mechanism the DNC actually used. It is immediately clear that the DNC's decision to use the stratified asssignment mechanism reduced the range of possible outcomes by eliminating some of the most skewed possibilities.

The blue bar in the graph above is the actual difference in total polling average across the two nights. Although the true difference of `r permutation_stat` is greater than the average difference of `r round(mean(simulated_data$block_dist),1)`, it is still well within. Although this visual test gives us a good sense of the result, we can be more precise. Below, I've plotted the empirical cumulative distribution function of the simulation results. 

```{r ecdf plot, fig.align = 'center'}
ecdf <- 
ggplot(data = simulated_data, aes(x = dist)) +
  stat_ecdf(geom = "step", color = 'gray30', size = 2) +
  geom_vline(xintercept = permutation_stat, color = 'blue', size=1.5) +
  theme_bw() +
  xlab('Difference in Total Polling Average') + ylab('') +
  ggtitle('Simple Random Assignment')

ecdf_block <- 
ggplot(data = simulated_data, aes(x = block_dist)) +
  stat_ecdf(geom = "step", color = 'gray30', size = 2) +
  geom_vline(xintercept = permutation_stat, color = 'blue', size=1.5) +
  theme_bw() +
  xlab('Difference in Total Polling Average') + ylab('') +
  ggtitle('Block Random Assignment')

ggpubr::ggarrange(ecdf, ecdf_block, ncol=2)
```

```{r ecdf}
P <- ecdf(simulated_data$block_dist)
```


We'd expect to see a result at least as extreme as the true lineups `r round((1-P(permutation_stat))*100, 1)`% of the time. In short, the DNC made a smart decision to use the blocked assignment mechanism, but got somewhat unlucky with the result. Although the lineup is at the outer edge of what we would expect, I think the media's characterization of the lineup as unbalanced or lopsided without comparing it to the underlying set of possibilites is unfortunate. It certainly reflects our [well-known inability to accurately perceive and assess randomness](https://www.sciencedirect.com/science/article/pii/019688589190029I), as well as a broader issues of statistical and probabalistic literacy in the media.
