---
title: "The Democratic Debate Lineup Isn't *that* Skewed"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
author: "Michael Fogarty"
date: "6/18/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

library(tidyverse)  ## general data manipulation
library(rvest)      ## web scraping
library(coin)       ## permutation test
library(knitr)      ## kable function
library(kableExtra) ## make it pretty

```

```{r read_data}
## Read in data -- scrape from 538

#Specifying the url for desired website to be scraped
url <- 'https://fivethirtyeight.com/features/the-dnc-tried-to-avoid-a-lopsided-debate-it-got-one-anyway/'

#Reading the HTML code from the website
webpage <- read_html(url)

scrape <- 
webpage %>% 
  html_nodes('#post-213083 > div > section > table') %>% 
  html_table() %>% 
  magrittr::extract2(1) %>% 
  janitor::clean_names() %>% 
  slice(1:10) %>% 
  mutate(avg = as.numeric(str_remove(avg, '%')),
         avg_2 = as.numeric(str_remove(avg_2, '%')))

## process the data - combine 2 columns into 1

night_1 <- 
scrape %>% select(june_26_debate, no_of_polls, avg) %>% 
  mutate(night = 'June 26') %>% 
  rename(candidate = june_26_debate)

night_2 <- 
  scrape %>% select(june_27_debate, no_of_polls, avg_2) %>% 
  mutate(night = 'June 27') %>% 
  rename(candidate = june_27_debate, avg = avg_2)

debates <- 
  bind_rows(night_1, night_2) %>% 
  mutate(night = as_factor(night))

rm(night_1, night_2, scrape, webpage, url)

simulated_data <- 
read_rds('/Users/michaelfogarty/Local Files/Other/Coding/R/R Practice/permutation_simulation_results.RDS')

```

```{r permutation_stat}
permutation_stat <- 
  debates %>% group_by(night) %>% summarize(total = sum(avg)) %>% 
  mutate(diff = abs(total - lag(total))) %>% 
  filter(!is.na(diff)) %>% select(diff) %>% magrittr::extract2(1)
```



## The Debate Lineups

Last Friday, the DNC released the lineups for the first two Democratic debates on June 26 and 27th. To avoid the backlash that followed the Republican primary debates in 2015 when the RNC divided its candidates into a top-tier and JV debate, the DNC decided to randomize the lineups for the two nights.

```{r lineup night 1}
debates %>% 
  filter(night == "June 26") %>% 
  select(candidate, avg) %>% 
  kable(col.names = c('Candidate', 'Polling Average'),
        caption = 'Night 1') %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), position = "center",
                full_width = F)
```

Elizabeth Warren stands out as the lone "top-tier" candidate on the first night (although a case could be made for Beto O'Rourke's status as a major contender as well), while the second debate is comparatively stacked with Joe Biden, Bernie Sanders, Kamala Harris and Pete Buttigieg.

```{r lineup night 2}
debates %>% 
  filter(night == "June 27") %>% 
  select(candidate, avg) %>% 
  kable(col.names = c('Candidate', 'Polling Average'),
        caption = 'Night 2') %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), position = "center",
                full_width = F)
```

News analysis and commentators have called attention to the apparent imbalance between the two nights. Nathaniel Rakich and Geoffrey Skelley at FiveThirtyEight called the debate [lopsided](https://fivethirtyeight.com/features/the-dnc-tried-to-avoid-a-lopsided-debate-it-got-one-anyway/). I argue that, while the lineups certainly aren't balanced, that this result is well within the range of what we'd expect, given that the candidates are assigned randomly to the two debates.

## Just How Imbalanced is this Lineup?

On the surface, it seems like there is a pretty big difference in the polling averages of the candidates on the first and second nights. The mean polling average for the candidates on the first night is `r debates %>% filter(night == 'June 26') %>%  summarize(mean(avg))`%, while the mean for the second night is `r debates %>% filter(night == 'June 27') %>%  summarize(mean(avg))`%. The total polling average (the sum of all of the candidates' averages for each night) also appear quite different: `r debates %>% filter(night == 'June 26') %>%  summarize(sum(avg))`% for the first night versus `r debates %>% filter(night == 'June 27') %>%  summarize(sum(avg))`% for the second.

However, the important question isn't whether or not this difference appears large or small out of context, but whether it is truly out of the range that we would expect given the random assignment of candidates to the two debates. To assess this, I used [permutation inference](https://thomasleeper.com/Rcourse/Tutorials/permutationtests.html), a statistical technique that builds the underlying distribution of possible debate lineups through simulation, then compared the actual outcome to the distribution of possible outcomes. Permutation tests are attractive because they don't make any assumptions about the underlying structure of the data and do not restrict the test statistic to a difference in means.

To measure how skewed the debate lineup is, I decided to focus on the difference in total polling average across the two nights (a similar analysis could be conducted for differences in means or medians as well). I simulated 10,000 random debate lineups and calculated the difference in total polling average between the two nights for each simulated lineup. 

```{r histogram, fig.align = 'center'}
ggplot(data = simulated_data, aes(x = dist)) + 
  geom_histogram(bins=100,  fill = 'gray30') +
  geom_vline(xintercept = permutation_stat, color = 'blue', size=1.5) +
  theme_bw() +
  xlab('Difference in Total Polling Average') + ylab('Count')
```

The blue bar in the graph above is the actual difference in total polling average across the two nights. Although the true difference of `r permutation_stat` is greater than the average difference of `r round(mean(simulated_data$dist),1)`, it is still well within the meat of the distribution of outcomes. Although this visual test gives us a good sense of the result, we can be more precise. Below, I've plotted the empirical cumulative distribution function of the simulation results. 

```{r ecdf plot, fig.align = 'center'}
ggplot(data = simulated_data, aes(x = dist)) +
  stat_ecdf(geom = "step", color = 'gray30', size = 2) +
  geom_vline(xintercept = permutation_stat, color = 'blue', size=1.5) +
  theme_bw() +
  xlab('Difference in Total Polling Average') + ylab('')
```

```{r ecdf}
P <- ecdf(simulated_data$dist)
```


We'd expect to see a result at least as extreme as the true lineups `r round((1-P(permutation_stat))*100, 1)`% of the time. It's unfair to call an outcome that happens one out of four times unbalanced or lopsided, but the media's reaction to the apparent disparity fits with our [well-known inability to accurately perceive and assess randomness](https://www.sciencedirect.com/science/article/pii/019688589190029I). Humans have a tendency to expect small samples to be representative of the true patterns or frequencies.

